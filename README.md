# Chicken Disease Classification Project
[![Ask DeepWiki](https://devin.ai/assets/askdeepwiki.png)](https://deepwiki.com/Spencer0013/Chicken-Disease-Classification-Project)

This repository contains a deep learning project for classifying chicken diseases (specifically Coccidiosis) based on fecal images. The project utilizes a Convolutional Neural Network (CNN), is managed with DVC for MLOps, and includes a Streamlit application for real-time predictions. Deployment is configured for Azure Web Apps using Docker and GitHub Actions.

## Table of Contents
1.  [Workflows for Development](#workflows-for-development)
2.  [Setup and Installation](#setup-and-installation)
3.  [Running the Application](#running-the-application)
4.  [Running the DVC Pipeline](#running-the-dvc-pipeline)
5.  [DVC Commands](#dvc-commands)
6.  [Project Structure](#project-structure)
7.  [MLOps Pipeline Stages](#mlops-pipeline-stages)
8.  [Deployment to Azure](#deployment-to-azure)
9.  [Technologies Used](#technologies-used)

## Workflows for Development
This outlines the general steps when modifying or extending the project:
1.  Update `config/config.yaml` (for paths, URLs, etc.)
2.  Update `secrets.yaml` (Optional, for sensitive credentials - ensure this is in `.gitignore`)
3.  Update `params.yaml` (for model hyperparameters, image sizes, etc.)
4.  Update the entity definitions in `src/cnnClassifier/entity/config_entity.py` if new configurations are added.
5.  Update the configuration manager in `src/cnnClassifier/config/configuration.py` to load new configurations.
6.  Update or add new components in `src/cnnClassifier/components/`.
7.  Update or add new pipeline stages in `src/cnnClassifier/pipeline/`.
8.  Update `main.py` if the overall DVC pipeline execution flow changes.
9.  Update `dvc.yaml` to reflect changes in stages, dependencies, or outputs.

## Setup and Installation

### Prerequisites
*   Conda (for environment management)
*   Git

### Steps
1.  **Clone the repository:**
    ```bash
    git clone https://github.com/Spencer0013/Chicken-Disease-Classification-Project.git
    cd Chicken-Disease-Classification-Project
    ```

2.  **Create and activate a Conda environment:**
    The project is configured for Python 3.10.
    ```bash
    conda create -n chicken-env python=3.10 -y
    conda activate chicken-env
    ```

3.  **Install the requirements:**
    This will install all necessary packages, including the local `cnnClassifier` package in editable mode.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Pull DVC tracked data (if available from a remote):**
    If large data files and models are tracked with DVC and stored in a remote storage, you might need to pull them.
    ```bash
    dvc pull
    ```
    (Note: This step assumes DVC remote storage has been configured. For initial setup or if data is generated by the pipeline, skip this and run the DVC pipeline.)

## Running the Application
The `app.py` file launches a Streamlit web application that allows you to upload a chicken fecal image and get a disease prediction (Healthy or Coccidiosis).

To run the Streamlit application:
```bash
streamlit run app.py
```
The application will typically be available at `http://localhost:8501`.

## Running the DVC Pipeline
The `main.py` script is used to execute the DVC pipeline stages sequentially as defined in `dvc.yaml`. This includes data ingestion, base model preparation, model training, and evaluation.

To run the entire MLOps pipeline:
```bash
python main.py
```
Alternatively, you can run DVC stages using `dvc repro`.

## DVC Commands
Data Version Control (DVC) is used to manage data, models, and experiments. Key commands include:
*   `dvc init`: Initialize DVC in the repository (should already be done).
*   `dvc repro`: Reproduce the pipeline by running stages defined in `dvc.yaml`. It intelligently re-runs only the changed stages.
*   `dvc dag`: Visualize the pipeline's Directed Acyclic Graph (DAG).

## Project Structure
```
├── artifacts/                  # Output files from DVC stages (models, processed data)
├── config/
│   └── config.yaml             # Configuration for paths, URLs
├── research/                   # Jupyter notebooks for experimentation
├── src/
│   └── cnnClassifier/          # Main source code for the classifier
│       ├── components/         # Individual pipeline components (data ingestion, training, etc.)
│       ├── config/             # Configuration management
│       ├── entity/             # Data entity definitions
│       ├── pipeline/           # DVC pipeline stage scripts and prediction pipeline
│       ├── utils/              # Utility functions
│       └── constants/          # Project constants (file paths)
├── templates/
│   └── index.html              # Basic HTML template (likely for future web app expansion)
├── .dvc/                       # DVC internal files
├── .github/workflows/          # GitHub Actions CI/CD workflows
├── app.py                      # Streamlit application for prediction
├── Dockerfile                  # Docker configuration for containerization
├── dvc.yaml                    # DVC pipeline definition
├── dvc.lock                    # DVC lock file, records versions of data/code/params
├── main.py                     # Script to run the DVC pipeline stages
├── params.yaml                 # Hyperparameters and model parameters
├── requirements.txt            # Python dependencies
├── setup.py                    # Setup script for packaging the cnnClassifier
├── scores.json                 # Model evaluation scores
└── README.md                   # This file
```

## MLOps Pipeline Stages
The pipeline is defined in `dvc.yaml` and orchestrated by DVC. The primary stages are:

1.  **`data_ingestion`**:
    *   Downloads chicken fecal image data from a specified URL.
    *   Unzips the downloaded data.
    *   Outputs: `artifacts/data_ingestion/Chicken-fecal-images`

2.  **`prepare_base_model`**:
    *   Loads a pre-trained VGG16 model.
    *   Modifies the top layers for binary classification (Healthy/Coccidiosis).
    *   Saves the base model structure.
    *   Outputs: `artifacts/prepare_base_model/base_model_updated.h5`

3.  **`training`**:
    *   Sets up callbacks (TensorBoard, ModelCheckpoint).
    *   Uses an ImageDataGenerator for training and validation data, with optional augmentation.
    *   Trains the prepared base model on the ingested data.
    *   Outputs: `artifacts/training/model.h5`

4.  **`evaluation`**:
    *   Loads the trained model.
    *   Evaluates the model on a validation set.
    *   Saves the loss and accuracy metrics.
    *   Outputs: `scores.json`

## Deployment to Azure
The project includes a GitHub Actions workflow (`.github/workflows/main_chickendiseaseprediction.yml`) for continuous integration and deployment (CI/CD) to Azure Web Apps.

### Deployment Steps (automated by GitHub Actions):
1.  **Build Docker Image**: On push to the `main` branch, a Docker image is built using the `Dockerfile`.
2.  **Push to Azure Container Registry (ACR)**: The built image is pushed to an Azure Container Registry instance.
3.  **Deploy to Azure Web App**: The Azure Web App is updated to pull and run the new Docker image from ACR.

### Docker Commands (for manual local testing/building):
*   Build the Docker image:
    ```bash
    docker build -t your-image-name .
    ```
*   Log in to a Docker registry (e.g., Azure Container Registry):
    ```bash
    docker login your-registry.azurecr.io
    ```
*   Push the image to the registry:
    ```bash
    docker push your-registry.azurecr.io/your-image-name:tag
    ```

## Technologies Used
*   **Python 3.10**
*   **TensorFlow & Keras**: For building and training the CNN model.
*   **Streamlit**: For creating the interactive web application.
*   **DVC (Data Version Control)**: For MLOps, managing data, models, and experiments.
*   **Pandas, NumPy, Matplotlib, Seaborn**: For data manipulation and visualization.
*   **Docker**: For containerizing the application.
*   **GitHub Actions**: For CI/CD automation.
*   **Azure Web Apps & Azure Container Registry**: For cloud deployment.
*   **Scikit-learn**: (Implicitly, often used with TensorFlow/Keras pipelines although not explicitly listed as a primary training component in the core path, but good for ML utilities).
*   **PyYAML, python-box**: For configuration management.